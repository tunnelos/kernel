; ------------------------------------------------------------
; ARMv8 MPCore EL3 AArch64 Startup Code
;
; Basic Vectors, MMU and caches initialization
;
; Exits in EL1 AArch64
;
; Copyright (c) 2014-2016 ARM Ltd.  All rights reserved.
; ------------------------------------------------------------


    .section StartUp, "ax"
    .balign 4


    .global el1_vectors
    .global el2_vectors
    .global el3_vectors

    .global InvalidateUDCaches
    .global ZeroBlock

    .global SetIrqGroup
    .global SetBlockGroup
    .global SetSPIGroup
    .global EnableIRQ
    .global TestIRQ
    .global EnableGICD
    .global EnableGICC
    .global SetIRQPriority
    .global SetPriorityMask
    .global ClearSGI

    .global __main
    .global MainApp

    .global Image$$EXEC$$RO$$Base
    .global Image$$TTB0_L1$$ZI$$Base
    .global Image$$TTB0_L2_RAM$$ZI$$Base
    .global Image$$TTB0_L2_PERIPH$$ZI$$Base
    .global Image$$TOP_OF_RAM$$ZI$$Base
    .global Image$$PRIVATE_PERIPHERALS$$ZI$$Base
    .global Image$$ARM_LIB_STACK$$ZI$$Limit
    .global Image$$EL3_STACKS$$ZI$$Limit
    .global Image$$CS3_PERIPHERALS$$ZI$$Base

    ; use separate stack and heap, as anticipated by scatter.txt
    .global __use_two_region_memory

    .include "asm/v8_mmu.s"
    .include "asm/v8_system.s"



; ------------------------------------------------------------

    .global start64
    .type start64, "function"
start64:

    ;
    ; program the VBARs
    ;
    ldr x1, =el1_vectors
    msr VBAR_EL1, x1

    ldr x1, =el2_vectors
    msr VBAR_EL2, x1

    ldr x1, =el3_vectors
    msr VBAR_EL3, x1

    ;
    ; set lower exception levels as non-secure, with no access
    ; back to EL2 or EL3, and are AArch64 capable
    ;
    mov x3, #(SCR_EL3_RW  | \
              SCR_EL3_SMD | \
              SCR_EL3_NS)
    msr SCR_EL3, x3

    ;
    ; no traps or VM modifications from the Hypervisor, EL1 is AArch64
    ;
    mov x2, HCR_EL2_RW
    msr HCR_EL2, x2

    ;
    ; VMID is still significant, even when virtualisation is not
    ; being used, so ensure VTTBR_EL2 is properly initialised
    ;
    msr VTTBR_EL2, xzr

    ;
    ; VMPIDR_EL2 holds the value of the Virtualization Multiprocessor ID. This is the value returned by Non-secure EL1 reads of MPIDR_EL1.
    ;  VPIDR_EL2 holds the value of the Virtualization Processor ID. This is the value returned by Non-secure EL1 reads of MIDR_EL1.
    ; Both of these registers are architecturally UNKNOWN at reset, and so they must be set to the correct value
    ; (even if EL2/virtualization is not being used), otherwise non-secure EL1 reads of MPIDR_EL1/MIDR_EL1 will return garbage values.
    ; This guarantees that any future reads of MPIDR_EL1 and MIDR_EL1 from Non-secure EL1 will return the correct value.
    ;
    ; keep MPIDR_EL1.Aff0 (i.e. the CPU no. on Cortex-A cores) in
    ; x19 (defined by the AAPCS as callee-saved), so we can re-use
    ; the number later
    ;
    mrs x0, MPIDR_EL1
    ubfx x19, x0, #MPIDR_EL1_AFF0_LSB, #MPIDR_EL1_AFF_WIDTH
    msr VMPIDR_EL2, x0

    mrs x0, MIDR_EL1
    msr VPIDR_EL2, x0

    ;
    ; neither EL3 nor EL2 trap floating point or accesses to CPACR
    ;
    msr CPTR_EL3, xzr
    msr CPTR_EL2, xzr

    ;
    ; SCTLR_EL3 comes out of reset with a limited set of known fields
    ; set; the only significant field is SCTLR_EL3.EE, which comes out
    ; of reset with an IMPLEMENTATION DEFINED value. Preserve SCTLR.EE
    ; and set the unknowns to 0 (all the other known values are already
    ;  0)
    ;
    mrs x0, SCTLR_EL3
    and x0, x0, #SCTLR_ELx_EE
    mrs x0, SCTLR_EL3

    ;
    ; set SCTLRs for lower ELs to safe values
    ;
    ; note that setting SCTLR_EL2 is not strictly
    ; needed, since we're never in EL2
    ;
    msr SCTLR_EL1, xzr
    msr SCTLR_EL2, xzr

#ifdef CORTEXA
    ;
    ; Configure ACTLR_EL[23]
    ; ----------------------
    ;
    ; These bits are IMPLEMENTATION DEFINED, so are different for
    ; different processors
    ;
    ; For Cortex-A57, the controls we set are:
    ;
    ;  Enable lower level access to CPUACTLR_EL1
    ;  Enable lower level access to CPUECTLR_EL1
    ;  Enable lower level access to L2CTLR_EL1
    ;  Enable lower level access to L2ECTLR_EL1
    ;  Enable lower level access to L2ACTLR_EL1
    ;
    mov x0, #((1 << 0) | \
              (1 << 1) | \
              (1 << 4) | \
              (1 << 5) | \
              (1 << 6))

    msr ACTLR_EL3, x0
    msr ACTLR_EL2, x0

    ;
    ; configure CPUECTLR_EL1
    ;
    ; These bits are IMP DEF, so need to different for different
    ; processors
    ;
    ; SMPEN - bit 6 - Enables the processor to receive cache
    ;                 and TLB maintenance operations
    ;
    ; Note: For Cortex-A57/53 SMPEN should be set before enabling
    ;       the caches and MMU, or performing any cache and TLB
    ;       maintenance operations.
    ;
    ;       This register has a defined reset value, so we use a
    ;       read-modify-write sequence to set SMPEN
    ;
    mrs x0, S3_1_c15_c2_1  ; Read EL1 CPU Extended Control Register
    orr x0, x0, #(1 << 6)  ; Set the SMPEN bit
    msr S3_1_c15_c2_1, x0  ; Write EL1 CPU Extended Control Register

    isb
#endif

    ;
    ; That's the last of the control settings for now
    ;
    ; Note: no ISB after all these changes, as registers won't be
    ; accessed until after an exception return, which is itself a
    ; context synchronisation event
    ;

    ;
    ; Setup some EL3 stack space, ready for calling some subroutines, below.
    ;
    ; Stack space allocation is CPU-specific, so use CPU
    ; number already held in x19
    ;
    ; 2^12 bytes per CPU for the EL3 stacks
    ;
    ldr x0, =Image$$EL3_STACKS$$ZI$$Limit
    sub x0, x0, x19, lsl #12
    mov sp, x0

    ;
    ; we need to configure the GIC while still in secure mode, specifically
    ; all PPIs and SPIs have to be programmed as Group1 interrupts
    ; this is a per-CPU configuration for these interrupts
    ;
    mov w0, #0
    mov w1, #1
    bl  SetBlockGroup

    ;
    ; While we're in the Secure World, set the priority mask low enough
    ; for it to be writable in the Non-Secure World
    ;
    mov w0, #16
    bl  SetPriorityMask

    ;
    ; there's more GIC setup to do, but only for the primary CPU
    ;
    cbnz x19, drop_to_el1

    ;
    ; There's more to do to the GIC - call the utility routine to set
    ; all SPIs to Group1
    ;
    mov w0, #1
    bl  SetSPIGroup

    ;
    ; Set up EL1 entry point and "dummy" exception return information,
    ; then perform exception return to enter EL1
    ;
    .global drop_to_el1
drop_to_el1:
    adr x1, el1_entry_aarch64
    msr ELR_EL3, x1
    mov x1, #(AARCH64_SPSR_EL1h | \
              AARCH64_SPSR_F  | \
              AARCH64_SPSR_I  | \
              AARCH64_SPSR_A)
    msr SPSR_EL3, x1
    eret



; ------------------------------------------------------------
; EL1 - Common start-up code
; ------------------------------------------------------------

    .global el1_entry_aarch64
    .type el1_entry_aarch64, "function"
el1_entry_aarch64:

    ;
    ; Now we're in EL1, setup the application stack
    ; the scatter file allocates 2^14 bytes per app stack
    ;
    ldr x0, =Image$$ARM_LIB_STACK$$ZI$$Limit
    sub x0, x0, x19, lsl #14
    mov sp, x0

    ;
    ; Enable floating point
    ;
    mov x1, #CPACR_EL1_FPEN
    msr CPACR_EL1, x1

    ;
    ; Invalidate caches and TLBs for all stage 1
    ; translations used at EL1
    ;
    ; Cortex-A processors automatically invalidate their caches on reset
    ; (unless suppressed with the DBGL1RSTDISABLE or L2RSTDISABLE pins).
    ; It is therefore not necessary for software to invalidate the caches 
    ; on startup, however, this is done here in case of a warm reset.
    bl  InvalidateUDCaches
    tlbi VMALLE1


    ;
    ; Set TTBR0 Base address
    ;
    ; The CPUs share one set of translation tables that are
    ; generated by CPU0 at run-time
    ;
    ; TTBR1_EL1 is not used in this example
    ;
    ldr x1, =Image$$TTB0_L1$$ZI$$Base
    msr TTBR0_EL1, x1


    ;
    ; Set up memory attributes
    ;
    ; These equate to:
    ;
    ; 0 -> 0b01000100 = 0x00000044 = Normal, Inner/Outer Non-Cacheable
    ; 1 -> 0b11111111 = 0x0000ff00 = Normal, Inner/Outer WriteBack Read/Write Allocate
    ; 2 -> 0b00000100 = 0x00040000 = Device-nGnRE
    ;
    mov  x1, #0xff44
    movk x1, #4, LSL #16    ; equiv to: movk x1, #0x0000000000040000
    msr MAIR_EL1, x1


    ;
    ; Set up TCR_EL1
    ;
    ; We're using only TTBR0 (EPD1 = 1), and the page table entries:
    ;  - are using an 8-bit ASID from TTBR0
    ;  - have a 4K granularity (TG0 = 0b00)
    ;  - are outer-shareable (SH0 = 0b10)
    ;  - are using Inner & Outer WBWA Normal memory ([IO]RGN0 = 0b01)
    ;  - map
    ;      + 32 bits of VA space (T0SZ = 0x20)
    ;      + into a 32-bit PA space (IPS = 0b000)
    ;
    ;     36   32   28   24   20   16   12    8    4    0
    ;  -----+----+----+----+----+----+----+----+----+----+
    ;       |    |    |OOII|    |    |    |00II|    |    |
    ;    TT |    |    |RRRR|E T |   T|    |RRRR|E T |   T|
    ;    BB | I I|TTSS|GGGG|P 1 |   1|TTSS|GGGG|P 0 |   0|
    ;    IIA| P P|GGHH|NNNN|DAS |   S|GGHH|NNNN|D S |   S|
    ;    10S| S-S|1111|1111|11Z-|---Z|0000|0000|0 Z-|---Z|
    ;
    ;    000 0000 0000 0000 1000 0000 0010 0101 0010 0000
    ;
    ;                    0x    8    0    2    5    2    0
    ;
    ; Note: the ISB is needed to ensure the changes to system
    ;       context are before the write of SCTLR_EL1.M to enable
    ;       the MMU. It is likely on a "real" implementation that
    ;       this setup would work without an ISB, due to the
    ;       amount of code that gets executed before enabling the
    ;       MMU, but that would not be architecturally correct.
    ;
    ldr x1, =0x0000000000802520
    msr TCR_EL1, x1
    isb

    ;
    ; x19 already contains the CPU number, so branch to secondary
    ; code if we're not on CPU0
    ;
    cbnz x19, el1_secondary

    ;
    ; Fall through to primary code
    ;


;
; ------------------------------------------------------------
;
; EL1 - primary CPU init code
;
; This code is run on CPU0, while the other CPUs are in the
; holding pen
;

    .global el1_primary
    .type el1_primary, "function"
el1_primary:

    ;
    ; We're now on the primary processor, so turn on
    ; the banked GIC distributor enable, ready for individual CPU
    ; enables later
    ;
    bl  EnableGICD

    ;
    ; Generate TTBR0 L1
    ;
    ; at 4KB granularity, 32-bit VA space, table lookup starts at
    ; L1, with 1GB regions
    ;
    ; we are going to create entries pointing to L2 tables for a
    ; couple of these 1GB regions, the first of which is the
    ; RAM on the VE board model - get the table addresses and
    ; start by emptying out the L1 page tables (4 entries at L1
    ; for a 4K granularity)
    ;
    ; x21 = address of L1 tables
    ;
    ldr x21, =Image$$TTB0_L1$$ZI$$Base
    mov x0, x21
    mov x1, #(4 << 3)
    bl  ZeroBlock

    ;
    ; time to start mapping the RAM regions - clear out the
    ; L2 tables and point to them from the L1 tables
    ;
    ; x0 = address of L2 tables
    ;
    ldr x0, =Image$$TTB0_L2_RAM$$ZI$$Base
    mov x1, #(512 << 3)
    bl  ZeroBlock

    ;
    ; Get the start address of RAM (the EXEC region) into x4
    ; and calculate the offset into the L1 table (1GB per region,
    ; max 4GB)
    ;
    ldr x4, =Image$$EXEC$$RO$$Base
    ldr x5, =Image$$TOP_OF_RAM$$ZI$$Base
    ubfx x2, x4, #30, #2

    orr x1, x0, #TT_S1_ATTR_PAGE
    str x1, [x21, x2, lsl #3]

    ;
    ; we've already used the RAM start address in x4 - we now need to get this
    ; in terms of an offset into the L2 page tables, where each entry covers 2MB
    ; x5 is the last known Execute region in RAM, convert this to an offset too,
    ; being careful to round up, then calculate the number of entries to write
    ;
    ubfx x2, x4, #21, #9
    sub  x3, x5, #1
    ubfx x3, x3, #21, #9
    add  x3, x3, #1
    sub  x3, x3, x2

    ;
    ; set x1 to the required page table attributes, then orr
    ; in the start address (modulo 2MB)
    ;
    ; L2 tables in our configuration cover 2MB per entry - map
    ; memory as Shared, Normal WBWA (MAIR[1]) with a flat
    ; VA->PA translation
    ;
    and x4, x4, 0xffffffffffe00000 ; start address mod 2MB
    mov x1, #(TT_S1_ATTR_BLOCK | \
             (1 << TT_S1_ATTR_MATTR_LSB) | \
              TT_S1_ATTR_NS | \
              TT_S1_ATTR_AP_RW_PL1 | \
              TT_S1_ATTR_SH_INNER | \
              TT_S1_ATTR_AF | \
              TT_S1_ATTR_nG)
    orr x1, x1, x4

    ;
    ; factor the offset into the page table address and then write
    ; the entries
    ;
    add x0, x0, x2, lsl #3

loop1:
    subs x3, x3, #1
    str x1, [x0], #8
    add x1, x1, #0x200, LSL #12    ; equiv to add x1, x1, #(1 << 21)  ; 2MB per entry
    bne loop1


    ;
    ; now mapping the Peripheral regions - clear out the
    ; L2 tables and point to them from the L1 tables
    ;
    ; The assumption here is that all peripherals live within
    ; a common 1GB region (i.e. that there's a single set of
    ; L2 pages for all the peripherals). We only use a UART
    ; and the GIC in this example, so the assumption is sound
    ;
    ; x0 = address of L2 tables
    ;
    ldr x0, =Image$$TTB0_L2_PERIPH$$ZI$$Base
    mov x1, #(512 << 3)
    bl  ZeroBlock

    ;
    ; get the PRIVATE_PERIPHERALS address into x4 and calculate
    ; the offset into the L1 table
    ;
    ldr x4, =Image$$PRIVATE_PERIPHERALS$$ZI$$Base
    ubfx x2, x4, #30, #2

    orr x1, x0, #TT_S1_ATTR_PAGE
    str x1, [x21, x2, lsl #3]

    ;
    ; there's only going to be a single 2MB region for PRIVATE_PERIPHERALS (in
    ; x4) - get this in terms of an offset into the L2 page tables
    ;
    ubfx x2, x4, #21, #9

    ;
    ; set x1 to the required page table attributes, then orr
    ; in the start address (modulo 2MB)
    ;
    ; L2 tables in our configuration cover 2MB per entry - map
    ; memory as NS Device-nGnRE (MAIR[2]) with a flat VA->PA
    ; translation
    ;
    and x4, x4, 0xffffffffffe00000 ; start address mod 2MB
    mov x1, #(TT_S1_ATTR_BLOCK | \
             (2 << TT_S1_ATTR_MATTR_LSB) | \
              TT_S1_ATTR_NS | \
              TT_S1_ATTR_AP_RW_PL1 | \
              TT_S1_ATTR_AF | \
              TT_S1_ATTR_nG)
    orr x1, x1, x4

    ;
    ; only a single L2 entry for this, so no loop as we have for RAM, above
    ;
    str x1, [x0, x2, lsl #3]

    ;
    ; we have CS3_PERIPHERALS that include the UART controller
    ;
    ; Get CS3_PERIPHERALS address into x4 and calculate the offset into the
    ; L2 tables
    ;
    ldr x4, =Image$$CS3_PERIPHERALS$$ZI$$Base
    ubfx x2, x4, #21, #9

    ;
    ; set x1 to the required page table attributes, then orr
    ; in the start address (modulo 2MB)
    ;
    ; L2 tables in our configuration cover 2MB per entry - map
    ; memory as NS Device-nGnRE (MAIR[2]) with a flat VA->PA
    ; translation
    ;
    and x4, x4, 0xffffffffffe00000 ; start address mod 2MB
    mov x1, #(TT_S1_ATTR_BLOCK | \
             (2 << TT_S1_ATTR_MATTR_LSB) | \
              TT_S1_ATTR_NS | \
              TT_S1_ATTR_AP_RW_PL1 | \
              TT_S1_ATTR_AF | \
              TT_S1_ATTR_nG)
    orr x1, x1, x4

    ;
    ; only a single L2 entry again - write it
    ;
    str x1, [x0, x2, lsl #3]

    ;
    ; issue a barrier to ensure all table entry writes are complete
    ;
    dsb ish

    ;
    ; Enable the MMU.  Caches will be enabled later, after scatterloading.
    ;
    mrs x1, SCTLR_EL1
    orr x1, x1, #SCTLR_ELx_M
    bic x1, x1, #SCTLR_ELx_A ; Disable alignment fault checking.  To enable, change bic to orr
    msr SCTLR_EL1, x1
    isb

    ;
    ; Branch to C library init code
    ;
    b  __main


; ------------------------------------------------------------

; AArch64 ARM C library startup add-in:

; The ARM Architecture Reference Manual for ARMv8-A states:
; "The ARMv8 AArch64 architecture permits instruction accesses to Non-cacheable Normal memory to be held in
; instruction caches. Correspondingly, the sequence for ensuring that modifications to instructions are available
; for execution must include invalidation of the modified locations from the instruction cache, even if the
; instructions are held in Normal Non-cacheable memory. This includes cases where the instruction cache is disabled."
;
; To invalidate the AArch64 instruction cache after scatter-loading and before initialization of the stack and heap,
; it is necessary for the user to:
;
; * Implement instruction cache invalidation code in _platform_pre_stackheap_init.
; * Ensure all code on the path from the program entry up to and including _platform_pre_stackheap_init is located
; in a root region. This includes __rt_entry from the AArch64 ARM C library. So the AArch64 ARM C library objects
; containing __rt_entry (__rtentry*.o) must be placed in a root region.
;
; In this example, this function is only called once, by the primary core

    .global _platform_pre_stackheap_init
    .type _platform_pre_stackheap_init, "function"
_platform_pre_stackheap_init:
    dsb ish     ; ensure all previous stores have completed before invalidating
    ic  ialluis ; I cache invalidate all inner shareable to PoU (which includes secondary cores)
    dsb ish     ; ensure completion on inner shareable domain   (which includes secondary cores)
    isb

    ; Scatter-loading is complete, so enable the caches here, so that the C-library's mutex initialization later will work
    mrs x1, SCTLR_EL1
    orr x1, x1, #SCTLR_ELx_C
    orr x1, x1, #SCTLR_ELx_I
    msr SCTLR_EL1, x1
    isb

    ret

; ------------------------------------------------------------
; EL1 - secondary CPU init code
;
; This code is run on CPUs 1, 2, 3 etc....
; ------------------------------------------------------------

    .global el1_secondary
    .type el1_secondary, "function"
el1_secondary:

    ;
    ; the primary CPU is going to use SGI 15 as a wakeup event
    ; to let us know when it is OK to proceed, so prepare for
    ; receiving that interrupt
    ;
    ; NS interrupt priorities run from 0 to 15, with 15 being
    ; too low a priority to ever raise an interrupt, so let's
    ; use 14
    ;
    mov w0, #15
    mov w1, #(14 << 1) ; we're in NS world, so adjustment is needed
    bl  SetIRQPriority

    mov w0, #15
    bl  EnableIRQ

    mov w0, #(15 << 1)
    bl  SetPriorityMask

    bl  EnableGICC

    ;
    ; wait for our interrupt to arrive
    ;

loop_wfi:
    dsb SY      ; Clear all pending data accesses
    wfi         ; Go to sleep

    ;
    ; something woke us from our wait, was it the required interrupt?
    ;
    mov w0, #15
    bl  TestIRQ
    cbz w0, loop_wfi

    ;
    ; it was - there's no need to actually take the interrupt,
    ; so just clear it
    ;
    mov w0, #15
    mov w1, #0        ; IRQ was raised by the primary CPU
    bl  ClearSGI

    ;
    ; Enable the MMU and caches
    ;
    mrs x1, SCTLR_EL1
    orr x1, x1, #SCTLR_ELx_M
    orr x1, x1, #SCTLR_ELx_C
    orr x1, x1, #SCTLR_ELx_I
    bic x1, x1, #SCTLR_ELx_A ; Disable alignment fault checking.  To enable, change bic to orr
    msr SCTLR_EL1, x1
    isb

    ;
    ; Branch to thread start
    ;
    B  main
